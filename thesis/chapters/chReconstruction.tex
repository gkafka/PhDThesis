\chapter{Event Reconstruction}
\label{ch:RecoCal}

The most basic form of raw data collected is a vector of hits above threshold. The MC simulation described in chapter \ref{ch:Simulation} also outputs events in this format. However, these objects by themselves are not very useful; instead, a certain level of reconstruction is required before real physics can be studied. The first major step in this process is to apply calibration so that the hits can be translated into a set of energy depositions, consistent throughout and across both detectors. Any number of algorithms can then be applied to create new objects or search for features, including tracks, the event vertex, or particle identifiers (PIDs). This chapter describes the calibration and elements of the reconstruction chain relevant to the NC disappearance analysis.

\section{Calibration}

The purpose of calibration is to ensure a uniform detector response throughout each and across both detectors. This is done in two major steps, a relative and absolute calibration. The relative calibration accounts for threshold effects and attenuation across a single cell. It is designed to create a uniform response throughout a cell and across a single detector. The absolute calibration creates a scale factor for each detector to convert the calibrated PE scale from the relative calibration into an energy unit. This section follows the SA notes in reference \cite{ref:TNCalib}.

The relative calibration is designed to convert the PE signal output from the electronics into a calibrated unit, such that two equal signals from any two detector locations mean equal true energy deposited. This part of the calibration accounts for threshold effects and attenuation in the WLS fibers, outputting a corrected PE value, the PECorr.

Hits from through-going cosmic ray muons, or muons that enter and exit the detector without stopping, are used for the relative calibration. The WindowTrack algorithm, a fast algorithm that fits straight lines through hits \cite{ref:RecoWinTrack}, is used to produce 3D tracks from the cosmic ray events, and only those with a successful reconstruction are used. Within these events, only tricell hits are used for the calibration procedure. A tricell hit is defined as a hit in cell $i$ within a given plane that also has hits in cells $i+1$ and $i-1$. Under special circumstances (low statistics, too many dead neighboring cells) different hits are used for a particular cell. The path length of the cosmic traveling through the cell and the distance from readout are calculated for all of the selected hits in a cell. The distance from readout is labeled $W$, an alias for either X or Y, such that $W = 0$ is the center of a cell and positive values of $W$ are closer to the readout. From this information, individual histograms of the average PE/cm vs $W$ are constructed for each cell. The relative calibration procedures apply corrections to these histograms.

The first effect handled by the relative calibration is of threshold and shielding. Thresholds refer to the issue that an energy deposition may not register for a hit at all, as opposed to simply being attenuated, if there are not enough photons that reach the APD. Shielding refers to the tendency of the detector mass to alter the average signal of a minimum ionizing particle, or MIP, as a function of distance to the readout. Both of these effects would bias the set of hits used by the calibration by preferably selecting hits with greater numbers of photons, in turn underestimating the true energy deposited in the cell. To account for this, a correction factor is applied for each cell,
\beq
T = \frac{PE}{\lambda} \cdot \frac{E_{True}}{E_{MIP}}
\label{eq:CalibThreshold}
\eeq

\n where $T$ is correction factor, $PE$ is the number of simulated photons that the electronics register, $\lambda$ is the number of photons that would be seen without fluctuations, $E_{True}$ is the true energy deposited in the scintillator, and $E_{MIP}$ is the energy that would be deposited based only on the particle path length through the cell. The ratio on the left accounts for the threshold correction since $\lambda$ is only dependent on the simulated threshold level, and the ratio on the right accounts for the shielding correction since $E_{MIP}$ is only depenedent on the path length. Two dimensional histograms of the correction factor as a function of the cell number and distance from the readout are made for each detector and view, then fit with a polynomial to remove noise. These histograms are then used to correct the corresponding data and MC. Figure \ref{fig:CalibThreshold} shows examples of this correction factor used for the FD.
\begin{figure}[htb]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=.47\textwidth]{figures/Calib/ThresholdFDX.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/ThresholdFDY.png} \\
  \end{tabular}
  \caption[Threshold and Shielding Correction Factors]{Correction factor for threshold and shielding effects at the FD as a function of cell number and distance from electronic readout. Cells in the X view are shown on the left, Y view on the right.}
  \label{fig:CalibThreshold}
\end{figure}

The next part of the relative calibration is the general form of the attenuation correction. Tricell hits are grouped by cell and fit to a double exponential to consider both short and long path light. For data, individual fits are performed for every single cell. In MC, a fit is performed on the group of all cells in a particular view and at the same location in a plane due to much lower simulated cosmic event statistics. The double exponential has the form
\beq
y = C + A\left(\exp\left(\frac{W}{X}\right) + \exp\left(-\frac{L+W}{X}\right)\right)
\label{eq:CalibAttenuation}
\eeq

\n where $C$, $A$, and $X$ are the free parameters, and $L$ is the full length of the cell. $X$ is the cell attenuation length. The fit excludes hits in the regions closest and furthest to the readout. It includes the range $[-750, 750]$ at the FD, $[-150, 150]$ for the fully active region of the ND and for Y view muon catcher cells, and $[-150, 50]$ for X view muon catcher cells, with all numbers in cm. These central regions are chosen to exclude the significant rolloff regions at the end of the cells, which are handled differently.

The final step in the relative calibration handles the attenuation correction at the ends of the cells and the residuals from the fit above in the central part of the cells. First, both of these regions are fit with a single LOWESS curve using a tricube weight,
\beq
w_i = \begin{cases}
\left(1 - \left| \frac{W - W_i}{\sigma} \right|^3 \right)^3 & \mbox{for } \vert W - W_i \vert < \sigma \\
0 & \mbox{for } \vert W - W_i \vert \geq \sigma \end{cases}
\label{eq:CalibRolloff}
\eeq

\n where $W$ is a local point on the curve, $W_i$ is the $i$th neighbor of the local point, $w_i$ is the weight on $W_i$, and $\sigma = 30\unit{cm}$ is the range of neighbors that affect the value of $W$. $W$ is then the weighted mean of $W_i$. Next, a simple line of the form $y = mW + c$ is fit to a collection of the neighbor points around a point, $W$, to give the corrected response, $y$. The full LOWESS curve is approximated by linear interpolation between $20$ points calculated using this procedure. The full attenuation calibration combines the results of the linear interpolation with the result of the double exponential above. Figure \ref{fig:CalibAttenuation} shows examples of the fully fit attenuation curves used in FD data calibration.
\begin{figure}[p]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=.47\textwidth]{figures/Calib/AttenuationFDH.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/AttenuationFDV.png} \\
  \end{tabular}
  \caption[Attenuation Fits]{Fits to the average detector response for a cell in the FD. The red curves show just the result from the double exponential fit, and the blue curves show the full fit that also includes the LOWESS correction. The left plot is an example of a horizontal view cell, the right plot is an example of a vertical view cell.}
  \label{fig:CalibAttenuation}
\end{figure}

The relative calibration combines the results of all three steps to create a uniform detector response as a function of distance to readout. Figure \ref{fig:CalibRelative} shows the MC before and after calibration is applied to assess the performance of the full procedure. The regions with deviations at the end of the cells ultimately are not used for the analysis.
\begin{figure}[p]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=.47\textwidth]{figures/Calib/RelativeNDX.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/RelativeNDY.png} \\
    \includegraphics[width=.47\textwidth]{figures/Calib/RelativeFDX.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/RelativeFDY.png} \\
  \end{tabular}
  \caption[Relative Calibration Results]{Ratios of mean reconstructed to true energy as a function of distance to readout to assess the performance of the relative calibration. The red points show the ratios before calibration; the blue points after. The left column shows calibration of the X view cells, the right column shows the Y view cells, the top row shows the ND relative calibration results, and the bottom shows the FD results.}
  \label{fig:CalibRelative}
\end{figure}

The absolute calibration is performed after the relative calibration to convert the corrected PE signal into an energy value. Much like the relative calibration, the absolute calibration uses a sample of tricell hits from cosmic ray muons. However, in this case the energy deposited in the cell must be known in order to convert the PE signal into an energy. The energy is known either from external sources \cite{ref:MuonTables} or the Bethe-Block equation \cite{ref:PDG},
\beq
\left\langle-\frac{dE}{dx} \right\rangle = Kz^2 \frac{Z}{A} \frac{1}{\beta^2} \left[ \frac{1}{2} \ln \frac{2 m_e c^2 \beta^2 \gamma^2 W_{max}}{I^2} - \beta^2 - \frac{\delta(\beta\gamma)}{2} \right],
\label{eq:BetheBlock}
\eeq

\n where $K \equiv 4\pi N_A r^2_e m_e c^2$, $r_e$ is the classical electron radius, $m_e c^2$ is the electron rest energy, $z$ is the charge number of the incident particle, $Z$ and $A$ are the atomic number and mass of the stopping material, $\beta$ and $\gamma$ are calculated for the incident particle, $W_{max}$ is the maximum energy transfer to an electron in a single collision, $I$ is the mean excitation energy of electrons in the stopping material, and $\delta(\beta\gamma)$ is the density effect correction to ionization energy loss in the stopping material. With known energy deposition and number of PEs, determining the calibration energy scale is a simple procedure.

The particular energy used for the absolute calibration is the minimum energy deposition of muons through liquid scintillator. For \nova, the liquid scintillator is well approximated as chains of polyethylene, or (C\textsubscript{2}H\textsubscript{4})\textsubscript{n}, for which muons have a minimum average energy loss of $2.079\unit{MeV}$ cm\textsuperscript{2}/g \cite{ref:MuonTables}. The density of the scintillator was measured to be $0.8617\unit{g/cm\textsuperscript{3}}$ \cite{ref:DensityScint}, giving an average energy loss per unit length of $1.792\unit{MeV/cm}$. From simulation, it was determined that this minimum energy deposition occurs between $100 - 200\unit{cm}$ from the end of a muon track, with only a $1.8\%$ variation throughout that region. Figure \ref{fig:CalibAbsDists} shows the distributions of the relative calibration corrected response as a function of distance from the muon track end for FD data and MC and the true energy deposition for the MC.
\begin{figure}[htb]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=.47\textwidth]{figures/Calib/AbsFDMCPECorrcm.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/AbsFDDataPECorrcm.png} \\
    \includegraphics[width=.47\textwidth]{figures/Calib/AbsFDMCdEdx.png} & \\
  \end{tabular}
  \caption[Detector Response to Stopping Cosmic Muons vs Distance to Track End]{Distribution of tricell hits from cosmic ray muon tracks at the FD as a function of distance to the end of the muon track. The top row shows the response in PECorr for both data and MC. The bottom left plot shows the true energy deposited for the MC.}
  \label{fig:CalibAbsDists}
\end{figure}

The absolute calibration energy scale is determined from distributions of tricell hits from stopping muons that occur between $100$ and $200\unit{cm}$ from the end of the muon track. These hits are used to make one dimensional muon energy unit (MEU) distributions of the relative calibration corrected detector response for data and MC called MEU\textsubscript{reco}, and the true energy deposition for MC called MEU\textsubscript{truth}. The calorimetric energy scale is then taken as the mean of the MEU\textsubscript{truth} distribution over the mean of the MEU\textsubscript{reco} distribution. Figure \ref{fig:CalibAbs} shows the distributions of the corrected detector response before the absolute calibration is applied, and the energy distributions after the absolute calibration is applied.
\begin{figure}[htb]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=.47\textwidth]{figures/Calib/DataMCNDPECorr.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/DataMCNDdEdx.png} \\
    \includegraphics[width=.47\textwidth]{figures/Calib/DataMCFDPECorr.png} &
    \includegraphics[width=.47\textwidth]{figures/Calib/DataMCFDdEdx.png} \\
  \end{tabular}
  \caption[Absolute Calibration Results]{Distributions of the tricell hits from cosmic ray muons $100-200\unit{cm}$ from the end of the track. The left column shows the relative calibration corrected detector response before the absolute calibration is applied. The right column shows the absolute calibrated energy deposition. The top row shows results for the ND; the bottom shows the FD.}
  \label{fig:CalibAbs}
\end{figure}

At this point the calibration procedure is complete. The calibration constants output by the relative and absolute calibrations are stored in a database so that the raw PE signal in any RawDigit object can be converted into an energy value.

\section{Reconstruction Chain}

The first step in the NC disappearance analysis is the selection of a relatively pure sample of NC events. However, the relatively raw calibrated hits are a far cry from the complete events necessary for the analysis. A set of reconstruction algorithms are thus applied to take individual hits and group them into more complete objects, often providing extra information along the way. The reconstruction procedures begin with the basic grouping of hits in spacetime, and become as complex as applying machine learning algorithms to separate events into different components. The NC disappearance analysis did not have an independent reconstruction chain from the two main \nova~analyses. Rather, the information used for the NC selection discussed in the next chapter was a mixture from the $\nue$ appearance and $\numu$ disappearance analyses. The remainder of this section discusses the reconstruction components relevant to the analysis described in this dissertation.

The first part step in reconstruction is the clustering of hits into separate events, or slices. This is done based on the Density-Based Spatial Clustering of Application with Noise (DBSCAN) algorithm \cite{ref:RecoDBSCAN}. The algorithm as applied to \nova~is described in detail in reference \cite{ref:ThesisMichael}; the main points are summarized here.

The DBSCAN algorithm uses a score function to compute a distance between neighboring points. A threshold is set to determine whether two neighbors are close. Points that have more than a set number of neighbors within a that distance are labeled {\em core points}, and the close neighbors of the core points that are not themselves considered core points are labeled {border points}. Clusters are formed by iterating over the hits in an event window, computing whether a point is a core point, then expanding to the neighbors of the core point, and moving to the next cluster when the current one is entirely bounded by border points. At the end of this procedure, slices with more than $3$ hits in each view are made into a `physics' slice, and any hits not assigned to a cluster are placed into a `noise' slice.

The score function used between two hits in the slicing algorithm is
\beq
\epsilon = \left( \frac{ \Delta T - \vert \Delta \vec{r}/c \vert }{ T_{res} } \right)^2 + \left( \frac{ \Delta Z }{ D_{pen} } \right)^2 + \left( \frac{ \Delta XY }{ D_{pen} } \right)^2 + \left( \frac{ PE_{pen} }{ PE } \right)^5,
\label{eq:SlicerScore}
\eeq

\n $T_{res}$ is the timing resolution of the hits added in quadrature, $D_{pen}$ is a distance penalty, $PE$ is the number of photoelectrons in both hits added in quadrature, and $PE_{pen}$ is a penalty on the number of photoelectrons. Hits that are in the same view vs opposite views are handled slightly differently. For hits in the same view, $\Delta \vec{r}$ is calculated in two dimensions. For hits in opposite views, $\Delta \vec{r}$ is one dimensional, $\Delta XY$ is 0, and $D_{pen}$ is replaced with a separate, smaller, opposite view plane penalty. $T_{res}$ was set individually for each hit during the timing calibration, discussed in reference \cite{ref:TNCalib}. The exponent on the PE term was set at $5$ as the PE spectrum for noise falls as $PE^{-2.5}$.

The free parameters and their values are summarized in Table \ref{tab:SlicerParams}. The parameters were tuned and the performance of the algorithm was measured based on two metrics, completeness, the percentage of energy deposited in the scintillator from a physics interaction included in a slice, and purity, the percentage of energy in a slice that came from a single physics interaction. Values were tuned separately for the ND and FD. Notably, $PE_{pen}$ was set to 0 for both detectors, effectively removing this term from the distance calculation.
\begin{table}[htb]
  \begin{center}
    \caption[Slicing Algorithm Free Parameters]{A summary of the free parameters used in the slicing algorithm, and the tuned values used for each detector. Table adapted from \cite{ref:ThesisMichael}.}
    \label{tab:SlicerParams}
    \begin{tabular}{c c c}
      \hline\hline
      Parameter & ND & FD \\
      \hline
      $\epsilon$, minimum score required to be considered close neighbors & $5.0$ & $2.0$ \\
      Minimum close neighbors required to be considered core point & $4$ & $4$ \\
      $D_{pen}$, distance penalty & $75.0$ & $100.0$ \\
      Opposite view plane penalty & $8$ & $4$ \\
      $PE_{pen}$, penalty on the PE total & $0$ & $0$ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

There are several reconstruction chains that begin with slices, each developed with a different philosophy. The $\nue$ reconstruction chain was designed to identify shower like objects and is focused on clustering particles. The $\numu$ reconstruction chain was designed to identify long tracks. Finally, a global machine learning algorithm was developed to identify and separate all types of events. The $\nue$ and $\numu$ reconstruction chains both terminate with machine learning algorithms as well, but they were not used for this analysis and so not discussed in detail here. The algorithms used in each of the three chains are discussed insofar that they are relevant to the analysis presented in this dissertation.

The $\nue$ reconstruction chain begins by searching for line like features in the slice, using a modified Hough Transform \cite{ref:RecoHough}. The algorithm as applied to \nova~is described in references \cite{ref:ThesisMichael, ref:TNHough}; it is summarized here.

The algorithm begins by constructing a Hough map with the parameters of lines between pairs of points. A separate map is constructed for the XZ and YZ views. The lines are parametrized in polar coordinates, $(\rho, \theta)$, with $\rho$ the perpendicular distance from the origin and $\theta$ the angle away from the X axis. For each line calculated, the Hough map is filled with a Gaussian smeared weight based on the line parameters,
\beqa
w &=& \exp \left( -\frac{(\rho - \rho_0)^2}{2\sigma^2_\rho} \right) \exp \left( -\frac{(\theta - \theta_0)^2}{2\sigma^2_\theta} \right) \label{eq:HoughWeight} \\
\sigma_\rho &=& \frac{3}{\sqrt{12}} \mbox{[cm]} \\
\sigma_\rho &=& \frac{3\unit{cm}}{d\sqrt{6}}
\eeqa

\n where $d$ is the distance between the two points in cm. There are two restrictions on the pairs of points that are evaluated. Any pairs of points that are farther than $\sqrt{15000}\unit{cm}$ apart are not evaluated together to limit the total number of contributions and limit contributions with very low uncertainties in $\theta$. Pairs of points that have the same X or Y coordinate are not evaluated together unless they are farther than $\sqrt{15000}/4\unit{cm}$ apart to avoid the creation of spurious horizontal or vertical lines. Finally, the Hough map is smoothed using a Gaussian smoothing weight over non-zero bins.

The algorithm then iterates over peaks in the Hough map to find and label valid lines. The highest peak is considered first. A $7\times7$ square of bins centered on the peak is used to find an average $(\rho, \theta)$, with the bin values weighted by the inverse distance from the central bin. The resultant parameters are recorded along with the size of the peak. The Hough map is then regenerated without hits within $6\unit{cm}$ of the line, with the exception of the most upstream and downstream hits. This process is then repeated to find the next line. For all iterations after the first, the current line is tested against previous lines. If the new line is within $15\unit{cm}$ in $\rho$ and $0.02\unit{rad}$ in $\theta$, the line is not added to the list of valid Hough lines, but the hits associated with the line are still removed. The algorithm terminates when there are no remaining peaks, or when there are $10$ Hough lines, whichever comes soonest.

The output of Hough Transform is the input to the Elastic Arms algorithm that is used to reconstruct the original neutrino interaction vertex. Elastic arms, also called deformable templates, is described in references \cite{ref:RecoElastic1, ref:RecoElastic2, ref:RecoElastic3, ref:RecoElastic4} and is designed to find outgoing tracks with a known vertex. The algorithm as applied to \nova~was modified to account for the unknown vertex, is fully described in reference \cite{ref:TNElastic}, and summarized here.

The Elastic Arms algorithm attempts to fit a set of arms, or tracks that originate at a single point, to the hits in the slice. Each arm, $a$, is parametrically described in spherical coordinates according to
\beqa
x(s) &=& x_0 + s \sin\theta_a \cos\phi_a \nonumber \\
y(s) &=& y_0 + s \sin\theta_a \sin\phi_a \nonumber \\
z(s) &=& z_0 + s \cos\theta_a
\label{eq:ElasticArms}
\eeqa

\n where $(x_0, y_0, z_0)$ is the neutrino event vertex, $s$ is the distance from the vertex, and $(\theta_a, \phi_a)$ are the standard spherical angles. The algorithm fits for values of $(x_0, y_0, z_0, \vec{\theta}, \vec{\phi})$ by minimizing an energy cost function,
\beq
E = \sum_{i=1}^N \sum_{a=1}^M V_{ia} M_{ia} + \lambda \sum_{i=1}^N \left( \sum_{a=1}^M V_{ia} - 1 \right)^2 + \frac{2}{\lambda_v} \sum_{a=1}^M D_a,
\label{eq:ElasticE}
\eeq
for an event with $N$ hits and $M$ arms, where $M_{ia}$ is the distance of hit $i$ to arm $a$, $V_{ia}$ is the strength of association of hit $i$ to arm $a$, $D_a$ is a penalty on the distance between the vertex and the first hit on arm $a$, and $\lambda$ and $\lambda_v$ control the penalty of the second and third terms, respectively. $M_{ia}$ is specifically computed as the perpendicular distance to the arm, unless the hit is in a backward direction with respect to the arm.
\beq
M_{ia} = \begin{cases}
\left( \frac{d_{ia}^{perp}}{\sigma_i} \right)^2 & \mbox{Standard} \\
\left( \frac{d_{i}^{vtx}}{\sigma_i} \right)^2 & \mbox{Hit in backward direction, } \frac{d_i^{vtx}}{\sigma_i} \leq 1 \\
\left( \frac{d_{i}^{vtx}}{\sigma_i} \right)^4 & \mbox{Hit in backward direction, } \frac{d_i^{vtx}}{\sigma_i} > 1
\end{cases}
\label{eq:ElasticM}
\eeq

\n Above, $\sigma_i$ is a measure of the spatial resolution of the cell, and set to $\sigma_i = 3/\sqrt{12}\unit{cm}$. $V_{ia}$ is calculated from
\beq
V_{ia} = \frac{e^{-\beta M_{ia}}}{e^{-\beta\lambda} + \sum_{b=1}^M e^{-\beta M_{ib}}},
\label{eq:ElasticV}
\eeq

\n where $\beta$ can be interpreted as a temperature via $\beta = 1/T$. For any hit, the sum of the association to all arms is bounded between $0$ and $1$, $0 < \sum_{a = 1}^M V_{ia} \leq 1$, with the difference from $1$ representing the probability that a hit is considered noise by this algorithm. This gives $\lambda$ the interpretation as the distance compared to $M_{ia}$ that a hit has a $50\%$ chance as being considered noise.

The penalty on the distance between the vertex and the first hit on the arm is not part of the general algorithm and is specific to \nova. Since there are two views, $D_a$ combines the distance to the first hit in both views, $D_a = d_a^{xz} + d_a^{yz}$. To calculate these distances, only hits with an above average association to an arm are used, as technically each hit has an association to every arm. This term in equation \ref{eq:ElasticE} was partially motivated by photons. The likelihood that a photon will travel a distance $d$ before converting into an $e^+e^-$ pair is proportional to $\exp(-d/\lambda_v)$, where $\lambda_v \approx (7/9) X_0$ and $X_0$ is the radiation length of the material the through which the photon is traveling. For the \nova~detectors, $X_0 \approx 39\unit{cm}$, or $6$ planes. This photon conversion length leads to an energy function error of
\beq
\chi^2 = -2\ln \mathcal{L} = 2\frac{d}{\lambda_v},
\label{eq:Elasticlv}
\eeq

\n hence the form of the penalty on this term in equation \ref{eq:ElasticE}.

Before searching for the best fit vertex and arm directions, the algorithm first considers the best seed from a set of vertex positions and directions. The number of arms is set as the larger of the number of quality Hough lines from the two views and generates a list of possible vertex seeds and arm direction seeds based on a sorted list of the hits in $z$ and from the Hough lines themselves. The algorithm then searches for the seed that minimizes the energy cost function. Once this is chosen, the best fit vertex and directions are found by minimizing equation equation \ref{eq:ElasticE}. Since this function is dependent on many parameters and can have many local minima, the algorithm starts with low $\beta$ or high $T$, allowing hits to have moderate association with many arms. The temperature is gradually lowered while searching for the best fit parameters, pushing hits to their correct arms in the process.

The results of the fit for the vertex position are summarized in table \ref{tab:Elastic}. It was found that the final arm directions were not very reliable, even while the vertex position was. Therefore, the vertex is the only output used from the Elastic Arms algorithm.
\begin{table}[htb]
  \begin{center}
    \caption[Results from Elastic Arms Vertex Fits]{Difference between the true and reconstructed vertex for Elastic Arms vertex fits to FD MC events. All numbers shown are in cm. Table adapted from \cite{ref:ThesisEvan}.}
    \label{tab:Elastic}
    \begin{tabular}{c c c c c c c c c}
      \hline\hline
      \multirow{2}{*}{Interaction} & \multicolumn{2}{c}{$\Delta x$} & \multicolumn{2}{c}{$\Delta y$} & \multicolumn{2}{c}{$\Delta z$} & \multicolumn{2}{c}{3D Resolution} \\
      & Mean & FWHM & Mean & FWHM & Mean & FWHM & Mean & FWHM \\
      \hline
      $\nue$ CC & $0.04$ & $4.11$ & $-0.02$ & $4.44$ & $0.73$ & $7.38$ & $10.65$ & $17.20$ \\
      $\numu$ CC & $0.11$ & $4.43$ & $-0.24$ & $4.61$ & $-0.61$ & $8.16$ & $11.56$ & $17.08$ \\
      NC & $0.35$ & $8.22$ & $-0.77$ & $7.84$ & $0.47$ & $9.50$ & $28.72$ & $30.80$ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

The reconstructed vertex output by Elastic Arms is used as the input to a clustering algorithm that determines the number of prongs are in the event, or tracks with an angular spread that originate at a single point, and the hits that belong to each prong. This algorithm is a possibilistic Fuzzy-K means clustering algorithm, described in detail in references \cite{ref:TNFuzzyK, ref:ThesisEvan} and summarized here.

The clustering algorithm is a Fuzzy-K Means algorithm extended to deal with an unknown number of clusters and noise. The classic Fuzzy-K Means \cite{ref:RecoFuzzy1, ref:RecoFuzzy2} takes an event origin, or vertex, and assumes that all activity radiating outward with tracks appearing as peaks in angular space. It searches for the optimal placement of $k$ clusters, or prongs, and associates activity, or hits, to them. Each hit is allowed to be associated to multiple prongs leading to the fuzzy nature of the prongs. However, they are subject to the constraint that all association strengths add to 1, meaning that every hit, including noise, will be associated to at least one prong. An extension of this algorithm \cite{ref:RecoFuzzyExt} removes the association normalization constraint and allows for a changing number of clusters. This allows noise hits to remain unassociated with any cluster, changing the probabilistic nature of the prong association to a possibilistic association.

The algorithm first proceeds by seeding the prongs, calculating the association of the hits to the prongs, then updating the prong centers. The prongs are seeded in angular space based on an activity density matrix,
\beq
w_k = \sum_{i=1}^n e^{- \left( \frac{\theta_k - \theta_i}{\sigma_i} \right)^2}
\label{eq:FuzzyDensity}
\eeq

\n with $360$ equally spaced bins from $-\pi$ to $\pi$. The hit uncertainty, $\sigma_i$, is given by
\beq
\sigma_i = \frac{1.745}{d} + 0.0204 + 0.000173d, \quad d < 5\unit{m},
\label{eq:FuzzySigma}
\eeq

\n where $d$ is the distance from the vertex to the hit. Equation \ref{eq:FuzzySigma} was modeled after the behavior of multiple scattering for muons but found to work well for all event types. This equation is used until $d = 5\unit{m}$, at which point the uncertainty is held constant. Once the density matrix is populated, the prongs are seeded by the densest bins. With the prongs seeded, the algorithm proceeds with calculating the membership of each hit. First, the angular distance between the hit and a cluster center is calculated,
\beq
d_{ij} = \left( \frac{ \theta_j - \theta_i }{\sigma_j} \right)^2,
\label{eq:Fuzzyd}
\eeq

\n where $j$ refers to the $j$\textsuperscript{th} hit and $i$ refers to the $i$\textsuperscript{th} cluster center. Next, the algorithm assigns membership to the cluster,
\beq
\mu_{ij} = e^{ - \frac{m d_{ij} \sqrt{c}}{\beta} },
\label{eq:Fuzzymu}
\eeq

\n where $m$ is the degree of fuzziness for the clusters, $c$ is the current number of clusters, and $\beta$ is a measure of the expected spread in the clusters. Both $m$ and $\beta$ are tunable parameters. When $m = 0$, the algorithm reduces to a hard clustering model where each hit belongs to a separate cluster. In \nova, $m$ is set to $2$ and $\beta$ to $4$. Finally, the cluster centers are updated.
\beq
\theta'_i = \theta_i + \frac{ \sum_{j = 1}^n \frac{\mu_{ij}^m}{\sigma_j^2} (\theta_j - \theta_i) }{ \sum_{j = 1}^n \frac{\mu_{ij}^m}{\sigma_j^2} }
\label{eq:FuzzyUpdate}
\eeq

\n This loop continues until none of the cluster centers update by more than a specified tolerance.

The next major component of the algorithm performs checks on the prongs and may modify the total number. If any cluster centers are found to have converged to the same value, the cluster centers are reseeded. The lower peak in the density matrix that led to a duplicate prong center is replaced with the next lowest peak, and the above procedure is repeated. If there are no valid peaks to replace the original seed, the total number of prongs is reduced by one but the algorithm does not repeat the above procedure. Once the cluster centers are stable and there are no duplicates, all the hits are tested for cluster membership. All hits that have $< 1\%$ membership to all clusters are used to populate a new density matrix and an extra cluster is added, seeded with the highest density bin from the new density matrix. All of the cluster centers are then updated through the above procedure, and this full process is repeated until all hits have at least $1\%$ membership to a cluster or when there are $7$ total prongs.

Lastly, the 2D prongs are matched across views to form 3D prongs. To do this, prongs from the XZ and YZ views that overlap for at least one plane are paired and their energy distributions are compared using a Kuiper test \cite{ref:Kuiper}. Specifically, the test considers the difference in the cumulative fractions of deposited energy between the two views as a function of distance through the prong, $s$.
\beqa
D^+(s) = E^{XZ}(s) - E^{YZ}(s) \nonumber\\
D^-(s) = E^{YZ}(s) - E^{XZ}(s)
\label{eq:FuzzyDs}
\eeqa

\n The final score of the Kuiper test maximizes each difference, and sums the result.
\beq
K = \mbox{max}(D^+(s)) + \mbox{max}(D^-(s))
\label{eq:Kuiper}
\eeq

\n The prongs with the lowest score are matched, all other pairs that include either of the 2D prongs are removed, and the process continues until there are no remaining possible matches.

The $\nue$ reconstruction chain finishes by creating shower objects and by training two particle identification algorithms (PIDs), but these are not used for the NC disappearance analysis.
